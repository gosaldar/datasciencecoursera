---
title: 'Machine Learning: Human Activity Recognition'
author: "Darwin Gosal"
date: "23 August 2015"
output: html_document
---

Background
==========

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, I will be using data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information of this dataset is available from the website here: http://groupware.les.inf.puc-rio.br/har

Environment
===========

In this project, I will be using R 3.2.1 on OSX with the following packages and set the random seed to 123.

```{r warning=FALSE,message=FALSE,results="hide"}
library(caret)
library(randomForest)
library(rattle)
library(rpart.plot)
set.seed(123)
```

Data Cleaning
=============

After reading the data, I removed the first seven columns because they are not relevant for our analysis.
I also remove colums where there are more the 50% of the data are NA.

```{r}
train <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!",""), stringsAsFactors=FALSE)
test  <- read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!",""), stringsAsFactors=FALSE)

train <- train[,-seq(1:7)]
train <- train[, colSums(is.na(train)) < nrow(train)*0.5]
train$classe<-as.factor(train$classe)
```

All this while making sure that the same set of variables are used for but training and test sets.
```{r}
myvars<-dput(names(train))
n=length(myvars)
myvars<-myvars[1:(n-1)]
test <- test[,myvars]
```

Before we do analysis, let's check for near zero variance features and drop them (in this case there is none)
```{r}
nzv <- nearZeroVar(train)
length(nzv)
if(length(nzv) > 0) train <- train[, -nzv]
```

Analysis
========

In our analysis, we will check for correclation among predictors and remove it at cut off point of 0.95. We then drop the highly correlated predictors from both training and test sets.
```{r}
corr=cor(train[,-53])
hc = findCorrelation(corr, cutoff=0.95) 
hc = sort(hc)
names(train)[c(hc)] 
reduced_train = train[,-c(hc)] 
reduced_train$classe=train$classe 
reduced_test = test[,-c(hc)] 
```

I split reduced_train dataset into training and cross validating set at 7:3 ratio.
```{r}
inTrain <- createDataPartition(y=reduced_train$classe, p=0.7, list=FALSE )
training <- reduced_train[inTrain,]
validating <- reduced_train[-inTrain,]
```

I will be training the datasets using two different methods, first is randomForest and second is Recursive Partitioning and Regresession Tree.
```{r}
cmodel = randomForest(classe~.,data=training)
predictrf <- predict(cmodel, newdata=validating)
confusionMatrix(predictrf,validating$classe)

pmodel = train(classe~.,method='rpart',data=training)
predictrpart <- predict(pmodel, newdata=validating)
confusionMatrix(predictrpart,validating$classe)
fancyRpartPlot(pmodel$finalModel)
```

From above, we can see that the model using randomForest, which give 99.41% accuracy or 0.59% out of sample error in the cross-validation set, is better than using rpart (accuracy of 53.44%), therefore we will used our model based of randomForest method to do prediction for our test dataset.

```{r}
answer<-predict(cmodel, newdata=reduced_test)
```

Submission
==========

Using the answer generated by our machine learning, we created the answer files that are submitted to Coursera sites. The result is correct for all 20 test cases.
```{r}
# code provided by Coursera
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(answer)
```


